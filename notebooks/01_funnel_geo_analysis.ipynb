"""
Synthetic Marketing & Sales Data Generator for Insure Co.

Generates realistic synthetic data representing marketing and sales performance
for a national insurance company across multiple channels and products.

Author: Michael Belli
Created for portfolio demonstration purposes.
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple
import random
import string

np.random.seed(42)
random.seed(42)

# Built-in name lists for generating realistic names
FIRST_NAMES = [
    # Traditional American / European origin
    'James', 'Mary', 'Robert', 'Patricia', 'John', 'Jennifer', 'Michael', 'Linda',
    'David', 'Elizabeth', 'William', 'Barbara', 'Richard', 'Susan', 'Joseph', 'Jessica',
    'Thomas', 'Sarah', 'Christopher', 'Karen', 'Charles', 'Lisa', 'Daniel', 'Nancy',
    'Matthew', 'Betty', 'Anthony', 'Margaret', 'Mark', 'Sandra', 'Donald', 'Ashley',
    'Steven', 'Kimberly', 'Paul', 'Emily', 'Andrew', 'Donna', 'Joshua', 'Michelle',
    'Kenneth', 'Dorothy', 'Kevin', 'Carol', 'Brian', 'Amanda', 'George', 'Melissa',
    'Timothy', 'Deborah', 'Ronald', 'Stephanie', 'Edward', 'Rebecca', 'Jason', 'Sharon',
    'Jeffrey', 'Laura', 'Ryan', 'Cynthia', 'Jacob', 'Kathleen', 'Gary', 'Amy',
    'Nicholas', 'Angela', 'Eric', 'Shirley', 'Jonathan', 'Anna', 'Stephen', 'Brenda',
    'Larry', 'Pamela', 'Justin', 'Emma', 'Scott', 'Nicole', 'Brandon', 'Helen',
    'Benjamin', 'Samantha', 'Samuel', 'Katherine', 'Raymond', 'Christine', 'Gregory', 'Debra',
    'Frank', 'Rachel', 'Alexander', 'Carolyn', 'Patrick', 'Janet', 'Jack', 'Catherine',
    'Dennis', 'Heather', 'Tyler', 'Diane', 'Aaron', 'Ruth', 'Adam', 'Olivia',
    'Nathan', 'Joyce', 'Henry', 'Virginia', 'Douglas', 'Victoria', 'Zachary', 'Kelly',
    'Peter', 'Lauren', 'Kyle', 'Christina', 'Noah', 'Joan', 'Ethan', 'Evelyn',
    'Jeremy', 'Judith', 'Walter', 'Megan', 'Christian', 'Andrea', 'Keith', 'Cheryl',
    'Roger', 'Hannah', 'Terry', 'Jacqueline', 'Austin', 'Martha', 'Sean', 'Gloria',
    'Gerald', 'Teresa', 'Carl', 'Ann', 'Harold', 'Sara', 'Dylan', 'Madison',
    'Arthur', 'Frances', 'Lawrence', 'Kathryn', 'Jordan', 'Janice', 'Jesse', 'Jean',
    'Bryan', 'Abigail', 'Billy', 'Alice', 'Bruce', 'Judy', 'Gabriel', 'Sophia',
    'Logan', 'Grace', 'Albert', 'Natalie', 'Willie', 'Theresa', 'Alan', 'Beverly',
    'Eugene', 'Denise', 'Russell', 'Marilyn', 'Vincent', 'Amber', 'Philip', 'Danielle',
    'Bobby', 'Brittany', 'Johnny', 'Diana', 'Bradley', 'Natasha', 'Connor', 'Brooke',
    'Mason', 'Kayla', 'Liam', 'Alexis', 'Aiden', 'Destiny', 'Jackson', 'Allison',
    'Sebastian', 'Savannah', 'Caleb', 'Haley', 'Owen', 'Mackenzie', 'Luke', 'Brooklyn',
    'Isaac', 'Peyton', 'Oliver', 'Riley', 'Carter', 'Aubrey', 'Hunter', 'Claire',
    'Elijah', 'Charlotte', 'Jayden', 'Zoey', 'Wyatt', 'Lillian', 'Levi', 'Addison',
    'Blake', 'Layla', 'Chase', 'Leah', 'Landon', 'Bella', 'Dominic', 'Zoe',
    'Gavin', 'Nora', 'Ian', 'Stella', 'Cameron', 'Violet', 'Nathaniel', 'Aurora',
    'Colton', 'Chloe', 'Parker', 'Penelope', 'Jaxon', 'Eleanor', 'Cole', 'Ellie',

    # Hispanic / Latino origin
    'Jose', 'Maria', 'Carlos', 'Rosa', 'Luis', 'Carmen', 'Miguel', 'Guadalupe',
    'Angel', 'Leticia', 'Francisco', 'Ana', 'Juan', 'Lucia', 'Antonio', 'Elena',
    'Pedro', 'Isabel', 'Alejandro', 'Sofia', 'Ricardo', 'Adriana', 'Fernando', 'Patricia',
    'Jorge', 'Claudia', 'Eduardo', 'Mariana', 'Rafael', 'Gabriela', 'Javier', 'Monica',
    'Oscar', 'Laura', 'Manuel', 'Veronica', 'Ruben', 'Diana', 'Raul', 'Alicia',
    'Sergio', 'Sandra', 'Diego', 'Yolanda', 'Roberto', 'Norma', 'Enrique', 'Silvia',
    'Arturo', 'Martha', 'Andres', 'Alejandra', 'Cesar', 'Rocio', 'Hector', 'Beatriz',
    'Victor', 'Marisol', 'Jesus', 'Esmeralda', 'Marco', 'Cristina', 'Mario', 'Liliana',

    'Darius', 'Aaliyah', 'Terrell', 'Imani', 'Darnell', 'Tamika', 'Jamal', 'Keisha',
    'Tyrone', 'Latoya', 'DeShawn', 'Ebony', 'Lamar', 'Jasmine', 'Terrence', 'Tiffany',
    'Antoine', 'Shaniqua', 'Dwayne', 'Tanisha', 'Jermaine', 'Lakisha', 'Marcus', 'Monique',
    'Andre', 'Shanice', 'Malik', 'Precious', 'DeAndre', 'Diamond', 'Kareem', 'Destiny',
    'Dante', 'Unique', 'Jamar', 'Tiara', 'Rasheed', 'Kiara', 'Cedric', 'Aliyah',
    'Quincy', 'Breanna', 'Leroy', 'Brianna', 'Reginald', 'Dominique', 'Tyree', 'Essence',
    'Wei', 'Mei', 'Jun', 'Lin', 'Chen', 'Xiu', 'Jian', 'Yan',
    'Ming', 'Hui', 'Feng', 'Fang', 'Lei', 'Ying', 'Cheng', 'Jing',
    'Hao', 'Hong', 'Bo', 'Ping', 'Tao', 'Qian', 'Kai', 'Shu',
    'Kenji', 'Yuki', 'Takeshi', 'Sakura', 'Hiroshi', 'Akiko', 'Masato', 'Haruka',
    'Yusuke', 'Emi', 'Ryu', 'Mika', 'Shin', 'Ayumi', 'Koji', 'Kumiko',
    'Raj', 'Priya', 'Amit', 'Anita', 'Vikram', 'Sunita', 'Sanjay', 'Neha',
    'Rahul', 'Pooja', 'Anil', 'Deepa', 'Suresh', 'Kavita', 'Vijay', 'Meena',
    'Ravi', 'Lakshmi', 'Arun', 'Anjali', 'Deepak', 'Shanti', 'Prakash', 'Rani',
    'Ashok', 'Rekha', 'Rajesh', 'Sita', 'Ramesh', 'Geeta', 'Sunil', 'Usha',
    'Arjun', 'Divya', 'Rohan', 'Shreya', 'Nikhil', 'Aishwarya', 'Karthik', 'Sneha',
    'Mohammed', 'Layla', 'Ahmed', 'Noor', 'Yusuf', 'Leila', 'Ibrahim', 'Yasmin',
    'Khalid', 'Amal', 'Mustafa', 'Dina', 'Samir', 'Hana', 'Rashid', 'Sara',
    'Tariq', 'Salma', 'Karim', 'Rania', 'Nabil', 'Mona', 'Walid', 'Lina',
    'Liam', 'Olivia', 'Noah', 'Emma', 'Oliver', 'Ava', 'Elijah', 'Sophia',
    'Lucas', 'Isabella', 'Mason', 'Mia', 'Ethan', 'Amelia', 'Aiden', 'Harper',
    'James', 'Evelyn', 'Benjamin', 'Abigail', 'Theo', 'Luna', 'Henry', 'Ella',
]

LAST_NAMES = [
    'Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Garcia', 'Miller', 'Davis',
    'Rodriguez', 'Martinez', 'Hernandez', 'Lopez', 'Gonzalez', 'Wilson', 'Anderson', 'Thomas',
    'Taylor', 'Moore', 'Jackson', 'Martin', 'Lee', 'Perez', 'Thompson', 'White',
    'Harris', 'Sanchez', 'Clark', 'Ramirez', 'Lewis', 'Robinson', 'Walker', 'Young',
    'Allen', 'King', 'Wright', 'Scott', 'Torres', 'Nguyen', 'Hill', 'Flores',
    'Green', 'Adams', 'Nelson', 'Baker', 'Hall', 'Rivera', 'Campbell', 'Mitchell',
    'Carter', 'Roberts', 'Gomez', 'Phillips', 'Evans', 'Turner', 'Diaz', 'Parker',
    'Cook', 'Rogers', 'Gutierrez', 'Ortiz', 'Morgan', 'Cooper', 'Peterson', 'Bailey',
    'Watson', 'Brooks', 'Chavez', 'Wood', 'James', 'Bennett', 'Gray', 'Mendoza',
    'Ruiz', 'Hughes', 'Price', 'Alvarez', 'Castillo', 'Sanders', 'Patel', 'Myers',
    'Sullivan', 'Bell', 'Coleman', 'Butler', 'Henderson', 'Barnes', 'Gonzales', 'Fisher',
    'Vasquez', 'Simmons', 'Stokes', 'Simpson', 'Crawford', 'Owens', 'Burns', 'Gordon',
    'Mason', 'Hunt', 'Hicks', 'Holmes', 'Rice', 'Robertson', 'Wheeler', 'Chapman',
    'Warren', 'Olson', 'Freeman', 'Webb', 'Tucker', 'Harrison', 'Wallace', 'Ford',
    'Gibson', 'Graham', 'Murray', 'Dean', 'Hayes', 'Stone', 'Meyer', 'Boyd',
    'Mills', 'Warren', 'Fox', 'Rose', 'Black', 'Shaw', 'Reynolds', 'Harper',
    'Knight', 'Elliott', 'Duncan', 'Hudson', 'Carroll', 'Lane', 'Riley', 'Armstrong',
    'Fernandez', 'Vargas', 'Aguilar', 'Romero', 'Herrera', 'Medina', 'Delgado', 'Vega',
    'Rios', 'Sandoval', 'Nunez', 'Soto', 'Mendez', 'Dominguez', 'Guerrero', 'Santiago',
    'Estrada', 'Acosta', 'Silva', 'Valdez', 'Cabrera', 'Campos', 'Padilla', 'Rojas',
    'Velasquez', 'Salazar', 'Contreras', 'Luna', 'Espinoza', 'Molina', 'Rivas', 'Cardenas',
    'Carrillo', 'Cervantes', 'Bautista', 'Ibarra', 'Miranda', 'Orozco', 'Trujillo', 'Villanueva',
    'Washington', 'Jefferson', 'Franklin', 'Banks', 'Booker', 'Brooks', 'Dixon', 'Gaines',
    'Grant', 'Hawkins', 'Hunter', 'Jefferson', 'Jordan', 'Lawrence', 'Lawson', 'Mack',
    'Wang', 'Li', 'Zhang', 'Liu', 'Chen', 'Yang', 'Huang', 'Zhao',
    'Wu', 'Zhou', 'Xu', 'Sun', 'Ma', 'Zhu', 'Hu', 'Guo',
    'Tanaka', 'Yamamoto', 'Watanabe', 'Suzuki', 'Takahashi', 'Ito', 'Nakamura', 'Kobayashi',
    'Kimura', 'Shimizu', 'Hayashi', 'Sato', 'Mori', 'Abe', 'Ikeda', 'Hashimoto',
    'Park', 'Choi', 'Jung', 'Kang', 'Cho', 'Yoon', 'Jang', 'Lim',
    'Tran', 'Le', 'Pham', 'Hoang', 'Vo', 'Dang', 'Bui', 'Do',
    'Ho', 'Ngo', 'Duong', 'Ly', 'Huynh', 'Truong', 'Luu', 'Dinh',
    'Patel', 'Shah', 'Kumar', 'Singh', 'Sharma', 'Gupta', 'Reddy', 'Nair',
    'Rao', 'Joshi', 'Mehta', 'Desai', 'Verma', 'Iyer', 'Kapoor', 'Malhotra',
    'Bhat', 'Chopra', 'Agarwal', 'Srinivasan', 'Venkatesh', 'Krishnan', 'Pillai', 'Menon',
    'Chaudhry', 'Mirza', 'Bukhari', 'Hashmi', 'Rizvi', 'Sheikh', 'Ansari', 'Siddiqui',
    'Hassan', 'Ali', 'Mohammed', 'Ibrahim', 'Abdullah', 'Rahman', 'Khalil', 'Saleh',
    'Bakr', 'Farooq', 'Hakim', 'Hamza', 'Ismail', 'Jamal', 'Kareem', 'Majid',
]

# make a class to be the name generator. Could have used a python package
class SimpleFaker:
    def first_name(self) -> str:
        return random.choice(FIRST_NAMES)

    def last_name(self) -> str:
        return random.choice(LAST_NAMES)


fake = SimpleFaker()

# =============================================================================
# CONFIGURATION
# =============================================================================

CONFIG = {
    # Time period
    'start_date': '2023-01-01',
    'end_date': '2025-12-31',

    # Target lead volume
    'total_leads': 100_000,

    # Products
    'products': ['Health', 'Life', 'Property_Casualty'],

    # Channels with their characteristics
    # Higher quality_score = better leads, higher cpl = cost per lead
    # Note: creates negative correlation between quality and cost (email cheap but low quality)
    'channels': {
        'paid_search': {
            'quality_score': 0.75,  # Highest quality
            'base_cpl': 45.0,  # Highest cost
            'lead_share': 0.45,  # 45% of leads
            'cpl_variance': 0.20,
        },
        'paid_social': {
            'quality_score': 0.55,  # Medium quality
            'base_cpl': 28.0,  # Medium cost
            'lead_share': 0.35,  # 35% of leads
            'cpl_variance': 0.25,
        },
        'email': {
            'quality_score': 0.35,  # Lowest quality (purchased list)
            'base_cpl': 8.0,  # Lowest cost
            'lead_share': 0.20,  # 20% of leads
            'cpl_variance': 0.15,
        }
    },

    # Base conversion rates (will be modified by channel quality and lead attributes)
    'base_conversion_rates': {
        'lead_to_qualified': 0.50,
        'qualified_to_quote': 0.60,
        'quote_to_binder': 0.30,
        'binder_to_sold': 0.88,
    },

    # Product-specific conversion modifiers
    'product_conversion_modifiers': {
        'Health': 1.05,  # Slightly higher conversion (mandatory coverage)
        'Life': 0.90,  # Lower conversion (discretionary)
        'Property_Casualty': 1.00,  # Baseline
    },

    # Demographics
    'income_brackets': ['<$30k', '$30-50k', '$50-75k', '$75-100k', '$100-150k', '$150k+'],
    'credit_scores': ['Poor', 'Fair', 'Good', 'Excellent'],

    # LTV ranges by product (will be modified by demographics)
    'base_ltv': {
        'Health': {'mean': 2800, 'std': 800},
        'Life': {'mean': 4500, 'std': 1500},
        'Property_Casualty': {'mean': 3200, 'std': 900},
    },

    # Email campaign settings
    'email_list_size': 20_000,
    'email_list_overlap': 0.75,  # 75% overlap year to year
    'email_conversion_rate': 0.005,  # 0.5%
    'emails_per_year': 26,  # Every other week

    # US States with population-weighted distribution
    'states': {
        'CA': 0.118, 'TX': 0.087, 'FL': 0.065, 'NY': 0.059, 'PA': 0.039,
        'IL': 0.038, 'OH': 0.035, 'GA': 0.032, 'NC': 0.031, 'MI': 0.030,
        'NJ': 0.028, 'VA': 0.026, 'WA': 0.023, 'AZ': 0.022, 'MA': 0.021,
        'TN': 0.021, 'IN': 0.020, 'MO': 0.018, 'MD': 0.018, 'WI': 0.017,
        'CO': 0.017, 'MN': 0.017, 'SC': 0.015, 'AL': 0.015, 'LA': 0.014,
        'KY': 0.013, 'OR': 0.013, 'OK': 0.012, 'CT': 0.011, 'UT': 0.010,
        'IA': 0.010, 'NV': 0.010, 'AR': 0.009, 'MS': 0.009, 'KS': 0.009,
        'NM': 0.006, 'NE': 0.006, 'ID': 0.006, 'WV': 0.005, 'HI': 0.004,
        'NH': 0.004, 'ME':
            0.004, 'MT': 0.003, 'RI': 0.003, 'DE': 0.003,
        'SD': 0.003, 'ND': 0.002, 'AK': 0.002, 'VT': 0.002, 'WY': 0.002,
        'DC': 0.002,
    }
}

# Seasonality patterns by product (monthly multipliers)
SEASONALITY = {
    'Health': [0.85, 0.80, 0.85, 0.90, 0.95, 0.95, 0.90, 0.95, 1.00, 1.10, 1.40, 1.35],
    'Life': [1.15, 1.10, 1.15, 1.20, 1.00, 0.90, 0.85, 0.85, 0.90, 0.95, 1.00, 0.95],
    'Property_Casualty': [0.90, 0.85, 1.05, 1.15, 1.20, 1.15, 1.05, 1.00, 0.95, 0.90, 0.85, 0.95],
}


# =============================================================================
# HELPER FUNCTIONS
# =============================================================================

def generate_email(first_name: str, last_name: str) -> str:
    """Generate a realistic email address."""
    domains = ['gmail.com', 'yahoo.com', 'hotmail.com', 'outlook.com', 'aol.com', 'icloud.com']
    separators = ['.', '_', '']

    sep = random.choice(separators)
    domain = random.choices(domains, weights=[0.45, 0.20, 0.10, 0.10, 0.08, 0.07])[0]

    patterns = [
        f"{first_name.lower()}{sep}{last_name.lower()}",
        f"{first_name.lower()}{sep}{last_name.lower()}{random.randint(1, 99)}",
        f"{first_name[0].lower()}{last_name.lower()}",
        f"{first_name.lower()}{random.randint(1, 999)}",
    ]

    username = random.choice(patterns)
    return f"{username}@{domain}"


def get_seasonality_multiplier(date: datetime, product: str) -> float:
    """Get seasonality multiplier for a given date and product."""
    month_idx = date.month - 1
    return SEASONALITY[product][month_idx]


def generate_demographic_profile(channel: str, product: str) -> Dict:
    """
    Generate demographic attributes for a lead.
    Higher quality channels produce leads with better demographics.
    """
    quality_score = CONFIG['channels'][channel]['quality_score']

    # Age distribution (skewed by product)
    if product == 'Life':
        age_mean = 42 + (quality_score * 8)  # Higher quality = slightly older, more established
        age_std = 12
    elif product == 'Health':
        age_mean = 38 + (quality_score * 5)
        age_std = 14
    else:  # Property_Casualty
        age_mean = 40 + (quality_score * 6)
        age_std = 13

    age = int(np.clip(np.random.normal(age_mean, age_std), 18, 80))

    # Income bracket (higher quality = higher income probability)
    income_weights = np.array([0.20, 0.25, 0.25, 0.15, 0.10, 0.05])
    # Shift weights toward higher income for better quality channels
    shift = int(quality_score * 2)
    income_weights = np.roll(income_weights, -shift)
    income_weights = income_weights / income_weights.sum()
    income_bracket = np.random.choice(CONFIG['income_brackets'], p=income_weights)

    # Credit score (higher quality = better credit probability)
    credit_weights = np.array([0.15, 0.30, 0.35, 0.20])
    # Shift weights toward better credit for higher quality
    credit_shift = int(quality_score * 1.5)
    credit_weights = np.roll(credit_weights, -credit_shift)
    credit_weights = credit_weights / credit_weights.sum()
    credit_score = np.random.choice(CONFIG['credit_scores'], p=credit_weights)

    return {
        'age': age,
        'income_bracket': income_bracket,
        'credit_score': credit_score,
    }


def calculate_conversion_probability(
        stage: str,
        channel: str,
        product: str,
        demographics: Dict
) -> float:
    """
    Calculate conversion probability based on channel quality,
    product type, and demographic attributes.
    """
    base_rate = CONFIG['base_conversion_rates'][stage]

    # Channel quality modifier
    quality_modifier = 0.7 + (CONFIG['channels'][channel]['quality_score'] * 0.6)

    # Product modifier
    product_modifier = CONFIG['product_conversion_modifiers'][product]

    # Demographic modifiers
    income_idx = CONFIG['income_brackets'].index(demographics['income_bracket'])
    income_modifier = 0.85 + (income_idx * 0.06)  # Higher income = slightly better conversion

    credit_idx = CONFIG['credit_scores'].index(demographics['credit_score'])
    credit_modifier = 0.80 + (credit_idx * 0.10)  # Better credit = better conversion

    # Combine modifiers
    final_prob = base_rate * quality_modifier * product_modifier * income_modifier * credit_modifier

    # Ensure probability stays in valid range
    return np.clip(final_prob, 0.05, 0.95)


def calculate_ltv(product: str, demographics: Dict, channel: str) -> float:
    """
    Calculate lifetime value based on product, demographics, and channel.
    Higher quality demographics = higher LTV.
    """
    base = CONFIG['base_ltv'][product]
    base_ltv = np.random.normal(base['mean'], base['std'])

    # Income modifier (higher income = higher LTV)
    income_idx = CONFIG['income_brackets'].index(demographics['income_bracket'])
    income_modifier = 0.7 + (income_idx * 0.15)

    # Credit modifier (better credit = higher LTV, lower churn)
    credit_idx = CONFIG['credit_scores'].index(demographics['credit_score'])
    credit_modifier = 0.8 + (credit_idx * 0.12)

    # Age modifier (middle-aged slightly higher LTV)
    age = demographics['age']
    if 35 <= age <= 55:
        age_modifier = 1.1
    elif 25 <= age < 35 or 55 < age <= 65:
        age_modifier = 1.0
    else:
        age_modifier = 0.9

    ltv = base_ltv * income_modifier * credit_modifier * age_modifier

    return round(max(ltv, 500), 2)  # Minimum LTV of $500


def simulate_funnel_progression(
        lead_date: datetime,
        channel: str,
        product: str,
        demographics: Dict
) -> Dict:
    """
    Simulate a lead's progression through the sales funnel.
    Returns timestamps for each stage reached (None if not reached).
    """
    stages = ['qualified', 'quote', 'binder', 'sold']
    stage_keys = [
        'lead_to_qualified',
        'qualified_to_quote',
        'quote_to_binder',
        'binder_to_sold'
    ]

    result = {
        'qualified_date': None,
        'quote_date': None,
        'binder_date': None,
        'sold_date': None,
        'final_status': 'lead',
        'ltv': None,
    }

    current_date = lead_date

    for i, (stage, stage_key) in enumerate(zip(stages, stage_keys)):
        prob = calculate_conversion_probability(stage_key, channel, product, demographics)

        if np.random.random() < prob:
            # Add realistic time delay between stages
            if stage == 'qualified':
                days_delay = np.random.randint(0, 3)
            elif stage == 'quote':
                days_delay = np.random.randint(1, 7)
            elif stage == 'binder':
                days_delay = np.random.randint(1, 14)
            else:  # sold
                days_delay = np.random.randint(1, 10)

            current_date = current_date + timedelta(days=days_delay)
            result[f'{stage}_date'] = current_date
            result['final_status'] = stage

            if stage == 'sold':
                result['ltv'] = calculate_ltv(product, demographics, channel)
        else:
            # Lead dropped off at this stage
            break

    return result


# =============================================================================
# DATA GENERATION FUNCTIONS
# =============================================================================

def generate_date_distribution(
        start_date: datetime,
        end_date: datetime,
        total_count: int,
        product_mix: Dict[str, float]
) -> List[Tuple[datetime, str]]:
    """
    Generate dates with seasonality patterns.
    Returns list of (date, product) tuples.
    """
    date_product_pairs = []

    # Calculate total days
    total_days = (end_date - start_date).days + 1

    # Generate base daily counts
    dates = [start_date + timedelta(days=i) for i in range(total_days)]

    for product, share in product_mix.items():
        product_count = int(total_count * share)

        # Calculate seasonality weights for each date
        weights = np.array([get_seasonality_multiplier(d, product) for d in dates])
        weights = weights / weights.sum()

        # Sample dates according to seasonality
        product_dates = np.random.choice(
            range(total_days),
            size=product_count,
            p=weights,
            replace=True
        )

        for day_idx in product_dates:
            date_product_pairs.append((dates[day_idx], product))

    # Shuffle to mix products
    random.shuffle(date_product_pairs)

    return date_product_pairs


def generate_email_list(year: int, previous_list: List[Dict] = None) -> List[Dict]:
    """
    Generate email list for a given year.
    Maintains overlap with previous year's list.
    """
    list_size = CONFIG['email_list_size']

    if previous_list is None:
        # First year - generate entirely new list
        email_list = []
        for _ in range(list_size):
            first_name = fake.first_name()
            last_name = fake.last_name()
            email_list.append({
                'first_name': first_name,
                'last_name': last_name,
                'email': generate_email(first_name, last_name),
                'added_year': year,
            })
    else:
        # Subsequent years - keep overlap, add new
        overlap_count = int(list_size * CONFIG['email_list_overlap'])
        new_count = list_size - overlap_count

        # Keep random selection from previous year
        email_list = random.sample(previous_list, overlap_count)

        # Add new contacts
        for _ in range(new_count):
            first_name = fake.first_name()
            last_name = fake.last_name()
            email_list.append({
                'first_name': first_name,
                'last_name': last_name,
                'email': generate_email(first_name, last_name),
                'added_year': year,
            })

    return email_list


def generate_email_campaigns(
        start_date: datetime,
        end_date: datetime
) -> Tuple[pd.DataFrame, List[Dict]]:
    """
    Generate email campaign data.
    Returns DataFrame of all emails sent and list of leads generated.
    """
    emails_sent = []
    email_leads = []

    # Generate email lists for each year
    email_lists = {}
    previous_list = None

    for year in range(start_date.year, end_date.year + 1):
        email_lists[year] = generate_email_list(year, previous_list)
        previous_list = email_lists[year]

    # Generate campaigns (every other week)
    campaign_dates = []
    current_date = start_date

    while current_date <= end_date:
        campaign_dates.append(current_date)
        current_date += timedelta(days=14)

    lead_id_counter = 1

    for campaign_date in campaign_dates:
        year = campaign_date.year
        email_list = email_lists[year]

        for contact in email_list:
            became_lead = np.random.random() < CONFIG['email_conversion_rate']

            lead_id = None
            if became_lead:
                lead_id = f"EMAIL_{lead_id_counter:06d}"
                lead_id_counter += 1

                # Determine product interest (random for email leads)
                products_interested = np.random.choice(
                    CONFIG['products'],
                    size=np.random.randint(1, 4),
                    replace=False
                ).tolist()

                email_leads.append({
                    'lead_id': lead_id,
                    'lead_date': campaign_date,
                    'channel': 'email',
                    'products': products_interested,
                    'first_name': contact['first_name'],
                    'last_name': contact['last_name'],
                })

            emails_sent.append({
                'email_id': f"EM_{campaign_date.strftime('%Y%m%d')}_{len(emails_sent):06d}",
                'campaign_date': campaign_date,
                'first_name': contact['first_name'],
                'last_name': contact['last_name'],
                'email': contact['email'],
                'became_lead': became_lead,
                'lead_id': lead_id,
            })

    return pd.DataFrame(emails_sent), email_leads


def generate_paid_channel_data(
        channel: str,
        start_date: datetime,
        end_date: datetime,
        target_leads: int
) -> Tuple[pd.DataFrame, pd.DataFrame, List[Dict]]:
    """
    Generate daily spend and lead data for paid channels.
    Returns daily_spend DataFrame, leads DataFrame, and list of lead dicts.
    """
    channel_config = CONFIG['channels'][channel]

    total_days = (end_date - start_date).days + 1
    dates = [start_date + timedelta(days=i) for i in range(total_days)]

    # Distribute leads across dates with some variance
    # More spend on weekdays, less on weekends
    daily_leads = []
    daily_spend = []
    leads_list = []

    lead_id_counter = 1
    prefix = 'SEARCH' if channel == 'paid_search' else 'SOCIAL'

    # Calculate average daily leads
    avg_daily_leads = target_leads / total_days

    for date in dates:
        # Weekend modifier
        weekend_modifier = 0.7 if date.weekday() >= 5 else 1.0

        # Monthly seasonality (aggregate across products)
        month_seasonality = np.mean([SEASONALITY[p][date.month - 1] for p in CONFIG['products']])

        # Random daily variance
        daily_variance = np.random.normal(1.0, 0.15)

        # Calculate leads for this day
        expected_leads = avg_daily_leads * weekend_modifier * month_seasonality * daily_variance
        actual_leads = max(0, int(np.random.poisson(expected_leads)))

        # Calculate spend
        base_cpl = channel_config['base_cpl']
        cpl_variance = channel_config['cpl_variance']
        actual_cpl = base_cpl * np.random.uniform(1 - cpl_variance, 1 + cpl_variance)

        # Spend includes some overhead beyond just lead generation
        total_spend = actual_leads * actual_cpl * np.random.uniform(1.1, 1.3)

        daily_spend.append({
            'date': date,
            'spend': round(total_spend, 2),
            'impressions': int(total_spend * np.random.uniform(80, 150)),
            'clicks': int(actual_leads * np.random.uniform(8, 15)),
            'leads': actual_leads,
            'cpl': round(total_spend / actual_leads, 2) if actual_leads > 0 else 0,
        })

        # Generate individual leads for this day
        for _ in range(actual_leads):
            lead_id = f"{prefix}_{lead_id_counter:06d}"
            lead_id_counter += 1

            # Determine products (can be multiple)
            num_products = np.random.choice([1, 2, 3], p=[0.65, 0.25, 0.10])
            products_interested = np.random.choice(
                CONFIG['products'],
                size=num_products,
                replace=False
            ).tolist()

            # Generate name for the lead
            first_name = fake.first_name()
            last_name = fake.last_name()

            daily_leads.append({
                'lead_id': lead_id,
                'date': date,
                'state': np.random.choice(
                    list(CONFIG['states'].keys()),
                    p=np.array(list(CONFIG['states'].values())) / sum(CONFIG['states'].values())
                ),
            })

            leads_list.append({
                'lead_id': lead_id,
                'lead_date': date,
                'channel': channel,
                'products': products_interested,
                'first_name': first_name,
                'last_name': last_name,
            })

    return pd.DataFrame(daily_spend), pd.DataFrame(daily_leads), leads_list


def generate_lead_detail_data(all_leads: List[Dict]) -> pd.DataFrame:
    """
    Generate detailed lead data with demographics and funnel progression.
    One row per lead-product combination.
    """
    lead_details = []

    for lead in all_leads:
        lead_id = lead['lead_id']
        lead_date = lead['lead_date']
        channel = lead['channel']
        products = lead['products']

        # Generate demographics once per lead (same person)
        demographics = generate_demographic_profile(channel, products[0])

        # Get state
        state = np.random.choice(
            list(CONFIG['states'].keys()),
            p=np.array(list(CONFIG['states'].values())) / sum(CONFIG['states'].values())
        )

        for product in products:
            # Simulate funnel for this lead-product combination
            funnel_result = simulate_funnel_progression(
                lead_date, channel, product, demographics
            )

            lead_details.append({
                'lead_id': lead_id,
                'product': product,
                'channel': channel,
                'state': state,
                'lead_date': lead_date,
                'first_name': lead.get('first_name', fake.first_name()),
                'last_name': lead.get('last_name', fake.last_name()),
                'age': demographics['age'],
                'income_bracket': demographics['income_bracket'],
                'credit_score': demographics['credit_score'],
                'qualified_date': funnel_result['qualified_date'],
                'quote_date': funnel_result['quote_date'],
                'binder_date': funnel_result['binder_date'],
                'sold_date': funnel_result['sold_date'],
                'final_status': funnel_result['final_status'],
                'ltv': funnel_result['ltv'],
            })

    return pd.DataFrame(lead_details)


# =============================================================================
# MAIN EXECUTION
# =============================================================================

def generate_all_data(output_dir: str = '.'):
    """
    Main function to generate all synthetic data files.
    """
    print("=" * 60)
    print("Insure Co. Synthetic Data Generator")
    print("=" * 60)

    start_date = datetime.strptime(CONFIG['start_date'], '%Y-%m-%d')
    end_date = datetime.strptime(CONFIG['end_date'], '%Y-%m-%d')

    total_leads = CONFIG['total_leads']

    # Calculate target leads by channel
    search_leads_target = int(total_leads * CONFIG['channels']['paid_search']['lead_share'])
    social_leads_target = int(total_leads * CONFIG['channels']['paid_social']['lead_share'])
    email_leads_target = int(total_leads * CONFIG['channels']['email']['lead_share'])

    print(f"\nGenerating data for {start_date.date()} to {end_date.date()}")
    print(f"Target leads: {total_leads:,}")
    print(f"  - Paid Search: {search_leads_target:,}")
    print(f"  - Paid Social: {social_leads_target:,}")
    print(f"  - Email: {email_leads_target:,}")

    all_leads = []

    # Generate Email Data
    print("\n[1/4] Generating email campaign data...")
    emails_df, email_leads = generate_email_campaigns(start_date, end_date)
    all_leads.extend(email_leads)
    print(f"  - Emails sent: {len(emails_df):,}")
    print(f"  - Leads from email: {len(email_leads):,}")

    # Generate Paid Search Data
    print("\n[2/4] Generating paid search data...")
    search_spend_df, search_leads_df, search_leads = generate_paid_channel_data(
        'paid_search', start_date, end_date, search_leads_target
    )
    all_leads.extend(search_leads)
    print(f"  - Total spend: ${search_spend_df['spend'].sum():,.2f}")
    print(f"  - Leads from search: {len(search_leads):,}")

    # Generate Paid Social Data
    print("\n[3/4] Generating paid social data...")
    social_spend_df, social_leads_df, social_leads = generate_paid_channel_data(
        'paid_social', start_date, end_date, social_leads_target
    )
    all_leads.extend(social_leads)
    print(f"  - Total spend: ${social_spend_df['spend'].sum():,.2f}")
    print(f"  - Leads from social: {len(social_leads):,}")

    # Generate Lead Detail Data
    print("\n[4/4] Generating lead detail data with funnel progression...")
    leads_detail_df = generate_lead_detail_data(all_leads)

    # Calculate summary statistics
    total_lead_product_combos = len(leads_detail_df)
    qualified_count = leads_detail_df['qualified_date'].notna().sum()
    quote_count = leads_detail_df['quote_date'].notna().sum()
    binder_count = leads_detail_df['binder_date'].notna().sum()
    sold_count = leads_detail_df['sold_date'].notna().sum()

    print(f"\n  Funnel Summary (lead-product combinations):")
    print(f"  - Total: {total_lead_product_combos:,}")
    print(f"  - Qualified: {qualified_count:,} ({qualified_count / total_lead_product_combos * 100:.1f}%)")
    print(f"  - Quoted: {quote_count:,} ({quote_count / total_lead_product_combos * 100:.1f}%)")
    print(f"  - Binder: {binder_count:,} ({binder_count / total_lead_product_combos * 100:.1f}%)")
    print(f"  - Sold: {sold_count:,} ({sold_count / total_lead_product_combos * 100:.1f}%)")
    print(f"  - Total LTV: ${leads_detail_df['ltv'].sum():,.2f}")

    # Save all files
    print("\n" + "=" * 60)
    print("Saving files...")

    # Leads detail file
    leads_file = f"{output_dir}/leads.csv"
    leads_detail_df.to_csv(leads_file, index=False)
    print(f"  ✓ {leads_file} ({len(leads_detail_df):,} rows)")

    # Search daily spend
    search_spend_file = f"{output_dir}/search_daily_spend.csv"
    search_spend_df.to_csv(search_spend_file, index=False)
    print(f"  ✓ {search_spend_file} ({len(search_spend_df):,} rows)")

    # Search leads
    search_leads_file = f"{output_dir}/search_leads.csv"
    search_leads_df.to_csv(search_leads_file, index=False)
    print(f"  ✓ {search_leads_file} ({len(search_leads_df):,} rows)")

    # Social daily spend
    social_spend_file = f"{output_dir}/social_daily_spend.csv"
    social_spend_df.to_csv(social_spend_file, index=False)
    print(f"  ✓ {social_spend_file} ({len(social_spend_df):,} rows)")

    # Social leads
    social_leads_file = f"{output_dir}/social_leads.csv"
    social_leads_df.to_csv(social_leads_file, index=False)
    print(f"  ✓ {social_leads_file} ({len(social_leads_df):,} rows)")

    # Emails sent
    emails_file = f"{output_dir}/emails_sent.csv"
    emails_df.to_csv(emails_file, index=False)
    print(f"  ✓ {emails_file} ({len(emails_df):,} rows)")

    print("\n" + "=" * 60)
    print("Data generation complete!")
    print("=" * 60)

    return {
        'leads': leads_detail_df,
        'search_spend': search_spend_df,
        'search_leads': search_leads_df,
        'social_spend': social_spend_df,
        'social_leads': social_leads_df,
        'emails': emails_df,
    }


if __name__ == "__main__":
    import os

    # Create output directory
    output_dir = "insure_co_data"
    os.makedirs(output_dir, exist_ok=True)

    # Generate all data
    data = generate_all_data(output_dir)

    print(f"\nAll files saved to: {output_dir}/")
